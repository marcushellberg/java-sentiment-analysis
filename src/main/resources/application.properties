server.port=${PORT:8080}
logging.level.org.atmosphere=warn
spring.mustache.check-template-location=false
vaadin.launch-browser=true
vaadin.allowed-packages=com.vaadin,org.vaadin,dev.hilla,com.example.application
spring.jpa.defer-datasource-initialization=true

### Classification example config

## NOTE: Use ONLY ONE of the two options below at a time.

# OpenAI API
# Better quality, requires sending data to OpenAI
langchain4j.open-ai.chat-model.api-key=${OPENAI_API_KEY}
langchain4j.open-ai.chat-model.model-name=gpt-4-turbo

# Local OpenAI compatible API (ollama)
# Not as performant, but your data does not leave your computer
#langchain4j.open-ai.chat-model.api-key=ollama
#langchain4j.open-ai.chat-model.base-url=http://localhost:11434/v1
#langchain4j.open-ai.chat-model.model-name=llama3:latest

langchain4j.open-ai.chat-model.temperature=0

# Debug logging to print requests
logging.level.dev.langchain4j=DEBUG
logging.level.dev.ai4j.openai4j=DEBUG
langchain4j.open-ai.chat-model.log-requests=true
#langchain4j.open-ai.chat-model.log-responses=true
